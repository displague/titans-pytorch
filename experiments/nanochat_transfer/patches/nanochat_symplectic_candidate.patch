diff --git a/nanochat/gpt.py b/nanochat/gpt.py
index 208acd1..50e7f84 100644
--- a/nanochat/gpt.py
+++ b/nanochat/gpt.py
@@ -37,6 +37,13 @@ class GPTConfig:
     # Characters: L=long (full context), S=short (half context)
     # Examples: "L"=all full context, "SL"=alternating, "SSL"=two short then one long
     window_pattern: str = "SSSL"
+    # Optional Titans-inspired token complexity gate (off by default).
+    symplectic_gate_enabled: bool = False
+    symplectic_gate_mix: float = 0.0
+    symplectic_gate_eps: float = 1e-6
+    symplectic_gate_odd_layers_only: bool = False
+    symplectic_gate_start_iter: int = 0
+    symplectic_gate_ramp_iters: int = 0
 
 
 def norm(x):
@@ -56,6 +63,25 @@ def apply_rotary_emb(x, cos, sin):
     y2 = x1 * (-sin) + x2 * cos
     return torch.cat([y1, y2], 3)
 
+
+def symplectic_token_gate(x, mix = 0.0, eps = 1e-6):
+    """
+    Lightweight tokenwise complexity gate.
+    Uses local finite differences as a proxy for "twist"/novelty.
+    Returns a gate in [1 - mix, 1], so behavior is unchanged when mix=0.
+    """
+    if mix <= 0.0 or x.size(1) <= 1:
+        return None
+
+    dx = x[:, 1:, :] - x[:, :-1, :]
+    energy = dx.pow(2).mean(dim = -1, keepdim = True)
+    first = energy[:, :1, :]
+    energy = torch.cat((first, energy), dim = 1)
+    norm_energy = energy / (energy.mean(dim = 1, keepdim = True) + eps)
+    complexity = torch.tanh(norm_energy)
+    gate = (1.0 - mix) + mix * complexity
+    return gate.clamp(min = 0.0, max = 1.0)
+
 class CausalSelfAttention(nn.Module):
     def __init__(self, config, layer_idx):
         super().__init__()
@@ -134,12 +160,31 @@ class MLP(nn.Module):
 class Block(nn.Module):
     def __init__(self, config, layer_idx):
         super().__init__()
+        self.layer_idx = layer_idx
         self.attn = CausalSelfAttention(config, layer_idx)
         self.mlp = MLP(config)
-
-    def forward(self, x, ve, cos_sin, window_size, kv_cache):
-        x = x + self.attn(norm(x), ve, cos_sin, window_size, kv_cache)
-        x = x + self.mlp(norm(x))
+        self.symplectic_gate_enabled = config.symplectic_gate_enabled
+        self.symplectic_gate_mix = config.symplectic_gate_mix
+        self.symplectic_gate_eps = config.symplectic_gate_eps
+        self.symplectic_gate_odd_layers_only = config.symplectic_gate_odd_layers_only
+
+    def forward(self, x, ve, cos_sin, window_size, kv_cache, gate_mix_override = None):
+        x_norm = norm(x)
+        attn_out = self.attn(x_norm, ve, cos_sin, window_size, kv_cache)
+        mlp_out = self.mlp(x_norm)
+
+        gate_mix = self.symplectic_gate_mix if gate_mix_override is None else gate_mix_override
+        use_gate = self.symplectic_gate_enabled and gate_mix > 0.0
+        if use_gate and self.symplectic_gate_odd_layers_only:
+            use_gate = (self.layer_idx % 2) == 1
+        if use_gate:
+            gate = symplectic_token_gate(x_norm, mix = gate_mix, eps = self.symplectic_gate_eps)
+            if gate is not None:
+                attn_out = attn_out * gate
+                mlp_out = mlp_out * gate
+
+        x = x + attn_out
+        x = x + mlp_out
         return x
 
 
@@ -184,6 +229,7 @@ class GPT(nn.Module):
         cos, sin = self._precompute_rotary_embeddings(self.rotary_seq_len, head_dim)
         self.register_buffer("cos", cos, persistent=False) # persistent=False means it's not saved to the checkpoint
         self.register_buffer("sin", sin, persistent=False)
+        self.register_buffer("symplectic_forward_step", torch.zeros((), dtype=torch.long), persistent=False)
 
     @torch.no_grad()
     def init_weights(self):
@@ -400,10 +446,24 @@ class GPT(nn.Module):
         x = self.transformer.wte(idx) # embed current token
         x = norm(x)
         x0 = x  # save initial normalized embedding for x0 residual
+        gate_mix_override = None
+        if self.config.symplectic_gate_enabled and self.config.symplectic_gate_mix > 0.0:
+            gate_mix_override = self.config.symplectic_gate_mix
+            if self.training:
+                start_iter = max(int(self.config.symplectic_gate_start_iter), 0)
+                ramp_iters = max(int(self.config.symplectic_gate_ramp_iters), 0)
+                step_idx = int(self.symplectic_forward_step.item())
+                if step_idx < start_iter:
+                    gate_mix_override = 0.0
+                elif ramp_iters > 0 and step_idx < (start_iter + ramp_iters):
+                    ramp_progress = float(step_idx - start_iter + 1) / float(ramp_iters)
+                    gate_mix_override = gate_mix_override * max(0.0, min(1.0, ramp_progress))
         for i, block in enumerate(self.transformer.h):
             x = self.resid_lambdas[i] * x + self.x0_lambdas[i] * x0
             ve = self.value_embeds[str(i)](idx) if str(i) in self.value_embeds else None
-            x = block(x, ve, cos_sin, self.window_sizes[i], kv_cache)
+            x = block(x, ve, cos_sin, self.window_sizes[i], kv_cache, gate_mix_override=gate_mix_override)
+        if self.training:
+            self.symplectic_forward_step.add_(1)
         x = norm(x)
 
         # Forward the lm_head (compute logits)
diff --git a/scripts/base_train.py b/scripts/base_train.py
index ccf35e6..11e3bae 100644
--- a/scripts/base_train.py
+++ b/scripts/base_train.py
@@ -51,6 +51,12 @@ parser.add_argument("--aspect-ratio", type=int, default=64, help="model_dim = de
 parser.add_argument("--head-dim", type=int, default=128, help="target head dimension for attention")
 parser.add_argument("--max-seq-len", type=int, default=2048, help="max context length")
 parser.add_argument("--window-pattern", type=str, default="SSSL", help="sliding window pattern tiled across layers: L=full, S=half context (e.g. 'SSL')")
+parser.add_argument("--symplectic-gate-enabled", action="store_true", help="enable optional Titans-inspired token complexity gate")
+parser.add_argument("--symplectic-gate-mix", type=float, default=0.0, help="mix for token complexity gate in [0,1]")
+parser.add_argument("--symplectic-gate-eps", type=float, default=1e-6, help="epsilon for token complexity gate stability")
+parser.add_argument("--symplectic-gate-odd-layers-only", action="store_true", help="apply token complexity gate only on odd-index transformer layers")
+parser.add_argument("--symplectic-gate-start-iter", type=int, default=0, help="step index at which token complexity gate starts applying")
+parser.add_argument("--symplectic-gate-ramp-iters", type=int, default=0, help="number of steps to linearly ramp gate mix after start iter")
 # Training horizon (only one used, in order of precedence)
 parser.add_argument("--num-iterations", type=int, default=-1, help="explicit number of optimization steps (-1 = disable)")
 parser.add_argument("--target-flops", type=float, default=-1.0, help="calculate num_iterations to reach target_flops (-1 = disable)")
@@ -80,6 +86,13 @@ parser.add_argument("--save-every", type=int, default=-1, help="save checkpoints
 parser.add_argument("--model-tag", type=str, default=None, help="override model tag for checkpoint directory name")
 args = parser.parse_args()
 user_config = vars(args).copy()  # for logging
+
+if not (0.0 <= args.symplectic_gate_mix <= 1.0):
+    raise ValueError("--symplectic-gate-mix must be in [0, 1]")
+if args.symplectic_gate_start_iter < 0:
+    raise ValueError("--symplectic-gate-start-iter must be >= 0")
+if args.symplectic_gate_ramp_iters < 0:
+    raise ValueError("--symplectic-gate-ramp-iters must be >= 0")
 # -----------------------------------------------------------------------------
 # Compute init and wandb logging
 
@@ -133,6 +146,12 @@ def build_model_meta(depth):
         sequence_len=args.max_seq_len, vocab_size=vocab_size,
         n_layer=depth, n_head=num_heads, n_kv_head=num_heads, n_embd=model_dim,
         window_pattern=args.window_pattern,
+        symplectic_gate_enabled=args.symplectic_gate_enabled,
+        symplectic_gate_mix=args.symplectic_gate_mix,
+        symplectic_gate_eps=args.symplectic_gate_eps,
+        symplectic_gate_odd_layers_only=args.symplectic_gate_odd_layers_only,
+        symplectic_gate_start_iter=args.symplectic_gate_start_iter,
+        symplectic_gate_ramp_iters=args.symplectic_gate_ramp_iters,
     )
     with torch.device("meta"):
         model_meta = GPT(config)
